# LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images
* XU, Ruyi, et al. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. arXiv preprint arXiv:2403.11703, 2024.
<br><br>

## [`논문 요약`]

### [저자의 의도]
* visual 인코딩은 large multimodal models(LMM)의 기본 구성 요소다.
* 기존 LMM들은 고정된 사이즈와 제한된 해상도로 이미지를 처리한다.
* 이는 적합성, 효율성, 정확도에 영향을 주는 고질적인 시스템적 문제이다.
* 어떤 종횡비의 고해상도 이미지가 들어와도 효율적으로 처리하는 메서드를 찾아보자.
* 이미지 모듈화, 압축 레이어, 공간 스키마 3가지 방법을 제시한다.
<br><br>

### [기존 문제점]
* 기존 LMM들은 다양한 종횡비 혹은 다양한 해상도의 이미지를 처리할 수 없다.
* 심지어 그게 1:1 종횡비의 224x224로 이미지가 블러처리 되거나 모양이 일그러진다.
* 세분화된 기능을 구현할 수 없고 할루시네이션 문제를 더 악화한다.
* 2가지 챌린지를 해결해야 가능해진다.
* Adaptivity, 인코더가 애초에 224x224로 트레이닝 됐다.
* Efficiency, ViT의 고질적인 문제로 이미지가 커지면 2차함수로 연산량이 증가한다.
* Pilot Experiments
    * GPT-4V는 가장 강력한 모델, LLaVA-1.5는 오픈 소스 중 가장 강력한 모델.
    * 실험을 통해 LMM의 시스템적 결함이 visual 인코딩에서 기인한 것을 증명해보자.
    * GPT-4V Experiments
        * OpenAI에서 제공하는 정보를 먼저 살펴보자.
        * GPT-4V는 이미지 처리에 저해상도 모드와 고해상도 모드 2개가 있다.
        * 저해상도 모드에서는 오직 저해상도 overview 이미지로 처리한다.
        * 고해상도 모드에서는 overview 이미지와 512x512 사이즈로 자른 슬라이스들을 함께 처리한다.
        * 이 고해상도 모드는 새로나온 것인데 흥미로운 에러 패턴이 있다.
        * 이미지에서 위치가 주는 영향 분석
            * Fig 1
            * Fig 1 (a) 이미지를 주고 '원이 몇개야?' 라고 물어본다.
            * 원의 위치를 바꿔가며 계속 똑같은 질문 프롬프트를 물어봤다.
            * 도형의 색깔(빨, 초, 흰)과 도형의 종류(원, 삼각, 사각)도 바꿔봤다.
            * Fig 1 (b) 각 포지션에 대해 15번 물어보고 평균값으로 히트맵을 그린 것이다.
            * 여기서 정답은 '4개' 이다.
            * 딱 봐도 큰 상관관계를 발견할 수 있다.
            * 256x256으로 자르면 3가지 패턴으로 나눌 수 있다.
            * 먼저 가운데 부분, 가장 높은 숫자를 대답.
            * 그리고 가운데 모서리, 1보다는 낮은 숫자를 대답.
            * 마지막으로 코너부분, 가장 ground truth와 비슷하게 대답.
            * 정답(4개)와 비슷한 답(3개, 5개)은 모든 영역에서 비슷하게 나왔다.
            * 근데 이상하게 2배(8개), 4배(16개)에 대한 대답이 나온다는 것이다.
            * 이런 패턴 때문에 (b)와 같은 히트맵이 나오는 것이다.
            * 결과 히트맵 (b)와 OpenAI의 정보를 종합한 가설은, 512 사이즈로 자르기 때문에 슬라이스들 간에 overlap이 발생하기 때문이다.
            * 이를 그림으로 나타내면 (e)와 같다.
            * 768사이즈의 이미지가 512로 딱 잘리지 않기 때문에 총 4개의 슬라이스가 생성될 것이다.
            * 따라서 2번 겹치는 부분은 2배를 4번 겹치는 부분을 4배를 대답하는 것으로 생각할 수 있다.
            * (Swin Transformer의 Shifted Window 냄새가 강하게 난다.)
        * 이미지에서 해상도가 주는 영향 분석
            * Fig 2
            * Fig 2 (a) 이미지를 주고 '원이 몇개야?' 라고 물어본다.
            * 이미지의 해상도를 바꿔가며 계속 똑같은 질문 프롬프트를 물어봤다.
            * 여기서 정답은 '9개' 이다.
            * Fig 2 (b), 이미지의 크기에 따라 3가지 페이즈로 볼 수 있다.
            * 페이즈1, 이미지를 슬라이스 하지 않으므로 정답인 9개의 빈도가 높다.
            * 페이즈2, 이미지가 슬라이스 되어 잘린 원에 의해 12개가 지배적이다.
            * 페이즈3, 슬라이스 때문에 9개, 12개, 16개가 섞여 나온다.
            * 이거 말고도 이상한 현상이 많은데 여기서 다루지는 않겠다.
    * LLaVA-1.5 Experiments
        * LLaVA는 일단 이미지를 정사각형으로 만들기 위해 패딩을 넣는다.
        * 일단 효율성 측면에서 패딩 픽셀을 연산하는 것은 비효율적이다.
        * Fig 3
        * 패딩의 다른 이슈는 패딩같은 회색 픽셀을 구분할 수 있는지 봐야한다.
        * 이미지의 종횡비를 바꿔가며 가장 왼쪽/오른쪽/위/아래 색이 무엇인지 질문했다.
        * LLaVA가 회색 픽셀을 패딩으로 착각하고 초록색이라고 답했다.
    * Conclusions on Pilot Experiments
        * 강력한 LMM인 GPT-4V와 LLaVA-1.5에 시스템적 결함이 존재한다.
        * 두 visual 인코딩 전략 말고 새로 디자인할 필요가 있다.
<br><br>

### [해결 아이디어]
* Fig 4
* Modularized Visual Encoding
    * 이미지를 잘라서 다양한 크기의 슬라이스로 만들어 인코딩 한다.
    * 기존 전략 2D linear interpolation
        * 다양한 종횡비를 처리하기 위해 ViT는 interpolation을 사용한다.
        * 예를 들면 ViT는 16 사이즈의 패치 (14x14개)가 디폴트이다.
        * 들어온 이미지를 패치로 쪼갰더니 (18x18개), (16x9개)와 같이 안맞는다.
        * (14x14개)의 위치 임베딩 격자점을 새로운 격자점에 맞게 대응시킨다.
        * 이때 사용하는 알고리즘은 2D linear interpolation 이다.
        * 이건 복잡한 수식은 없고 linear projection에 가까운 알고리즘이다.
        * 그런데 이 접근법은 quadratic하게 증가하는 연산량과 성능 저하를 가져온다.
    * 새로운 전략
        * 들어오는 이미지를 다양한 크기의 슬라이스 조각으로 자른다.
        * 이 슬라이스의 모양이 ViT가 학습했던 크기에서 크게 벗어나지 않게 한다.
        * 패딩도 필요없고 형태가 왜곡되는 reshape을 하지 않아도 된다.
    * High-resolution image partition strategy
        * 최적의 슬라이스를 찾는 방법이 무엇인지 보자.
        * 입력 이미지의 해상도(W_I, H_I)
        * ViT의 프리트레이닝 해상도(W_v, H_v)
        * 슬라이스의 총 개수(N), 슬라이스의 컬럼 수(m), 로우 수(n)
        * N을 먼저 구한 뒤 이미지를 쪼갤 수 있는 경우의 수 C를 구한다.
        * 스코어 함수 S는 Equation 1 이다.
        * 프리트레이닝 종횡비와 비슷할 수록 S가 높아진다.
        * 따라서 S가 높을수록 최적의 슬라이스이다.
        * Equation 2는 소수와 인수분해를 이용한다.
        * 더 최적의 슬라이스를 선택하는 수학적인 방법에 대한 설명이다.
    * Arbitrary aspect ratio slice encoding
        * 고정된 슬라이스 사이즈 때문에 종횡비에 대한 적응력이 매우 부족하다.
        * 원래 이미지의 종횡비와 비례하게 리사이즈 한다.
        * ViT 패치의 사이즈로 딱 맞아 떨어질 만큼 이미지를 키운다.
        * ViT의 1D 포지션 임베딩을 2D 포멧 P로 바꾼다. (차원 l 추가)
        * 그리고 나서 2D interpolation으로 P를 처리한다.
        * overview 이미지 역시 기존 종횡비 그대로 넣는다.
* Compression layer
    * 압축 레이어에 의해 visual 토큰이 적당한 길이로 응축된다.
    * 고해상도 이미지는 LLM에 엄청난 양의 visual 토큰 처리를 요구한다.
    * 이를 해결하기 위해 각각의 이미지 슬라이스의 토큰을 압축한다.
    * shared perceiver resampler layer를 사용한다.
    * 크로스 어텐션을 통해 쿼리 벡터 세트를 사용한다.
    * 이 논문에서는 이미지 토큰의 수를 576개에서 64개로 줄였다.
    * 리샘플러 방식은 해상도와 관계없이 고정된 수의 visual 토큰을 유지한다.
    * 따라서 MLP를 사용해서 압축하는 방법보다 더 낫다.
* Spatial schema
    * 응축된 슬라이스 토큰을 공간 스키마에 정리해서 위치가 어딘지 파악하게 해준다.
    * 공간 스키마를 디자인해 이미지 슬라이스들의 상대적 위치를 알려준다.
    * 콤마와 줄바꿈을 사용해서 슬라이스 representation의 행렬을 구분한다.
<br><br>

### [결과 분석]
* Implementation details
    * LLaVA-1.5를 개조한 LLaVA-UHD를 사용한다.
    * CLIP-ViT-L/14가 visual 인코더로 사용된다.
    * Vicuna-13B이 shared visual resampler로 사용된다.
    * 트레이닝 스테이지 1, visual resampler 학습, CC-595K 데이터셋, 1 epoch, AdamW
    * 트레이닝 스테이지 2, visual 인코더 frozen, visual resampler, LLM 학습, 656K mixture 데이터셋(LLaVA-Instruct, TextVQA, GQA, OCR-VQA, Visual Genome)
* Main results
    * Table 1
    * LLaVA-UHD가 대부분의 벤치마크에서 가장 뛰어났다.
    * 심지어 Qwen-VL, InstructBLIP 처럼 추가 데이터셋을 사용한 경우와 1B이 넘는 모델들 보다도 좋은 성능이다.
    * LLaVA-1.5 backbone의 성능을 크게 향상시켰다.
    * 인퍼런스 연산량은 줄면서 더 큰 해상도의 모든 종횡비의 이미지를 처리할 수 있다.
* Ablation study
    * Table 2
    * LLaVA-1.5의 패딩 전략을 LLaVA-UHD의 adaptive 방식으로 변경해봤다.
    * LLaVA-UHD의 resampler 방식을 MLP 방식으로 변경해봤다.
    * FP는 파티션 전략을 추가한 것이다.
    * spatial schema를 제거해봤다.
    * MLP가 아닌 resampler를 사용하면 FLOPs가 엄청나게 줄어든다.
    * 다른 케이스들은 퍼포먼스 감소가 있었다.
* 결론
    * 고해상도의 모든 종횡비를 처리할 수 있는 LMM을 제시했다.
    * LLaVA-UHD는 672x1008에서 한계였다.
    * efficiency와 scalability 연구를 통해 더 큰 해상도를 처리해야 한다.
    * 이미지 슬라이스들은 여전히 독립적으로 인코딩 되는데 이를 효율적으로 연결하거나 한번에 인코딩하는 새로운 전략이 필요하다.
<br><br>

### [추가로 볼 레퍼런스]
* 
<br><br>

### [내 아이디어]
* 
<br><br>



## [`메모`]

### [배경 지식]
* 
<br><br>


