# LLaMA: Open and Efficient Foundation Language Models
* TOUVRON, Hugo, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
<br><br>

## [`논문 요약`]

### [저자의 의도]
* 7B ~ 65B 크기의 foundation language 모델을 만든다.
* 독점적이고 비공개된 데이터가 아니라 오픈 데이터로 SOTA를 달성해보자.
* 저자들은 리서치 커뮤니티에 모델의 모든 정보를 공개했다.
<br><br>

### [기존 문제점]
* 요즘 LLM 연구들은 모델 사이즈가 성능에 비례한다는 가정을 가진다.
* 기존 모델들은 학습 예산만 고려하고 추론 예산에 대하여 고려하지 않는다.
* 친칠라, PaLM, GPT-3는 본인들만 접근할 수 있는 클로즈 데이터셋을 사용한다.
<br><br>

### [해결 아이디어]
* 1T 토큰 모델과 학습 예산
    * 친칠라 논문을 먼저 분석해본다.
    * 더 작은 사이즈의 모델도 더 많은 데이터로 학습하면 성능이 좋다. 
    * `학습 예산`에서 모델 사이즈와 데이터셋 사이즈의 최적의 조합을 찾는다.
    * 저자들은 이 논문이 `추론 예산`를 무시하고 있다고 말한다.
    * 성능의 타겟 레벨을 고정했을 때, 학습이 빠른 모델보다 추론이 빠른 모델이 더 좋다.
    * 비록 학습이 빠른 모델은 연구에서 비용이 싸지만, 추론이 빠른 모델이 서비스에서 더 싸다.
    * 서비스에서 더 싼 것이 궁극적으로 더 유리할 수 있다.
    * 이 관점에서 기존 모델보다 더 많은 토큰으로 훈련해보자.
    * 토큰 수를 늘린 모델으로 다양한 `추론 예산`에서 최상의 성능 모델을 훈련한다.
* overview
    * 대규모 데이터셋에 대하여 대규모 트랜스포머를 기존 옵티마이저를 사용해 학습한다.
* Pre-training Data
    * 
<br><br>

### [결과 분석]
* 
<br><br>

### [추가로 볼 레퍼런스]
* 
<br><br>

### [내 아이디어]
* 
<br><br>



## [`메모`]

### [배경 지식]
* 
<br><br>


