# LLM RLHF 기법 정리 (PPO, DPO, IPO, KTO, ORPO, GRPO 총정리)
* [`수식 분석`](https://chatgpt.com/c/686c786e-6130-800b-926b-7189f7a88499)
* [`데이터셋 예시`](https://claude.ai/chat/46765b45-fa8a-408e-a659-e7052eb3e64e)
<br><br>

## `[개요]`

### [LLM 최적화 기법]
* LLM은 사전학습 이후에 다양한 방법으로 후속 최적화를 진행한다.
* 대표적으로 `SFT`와 `RLHF` 2가지가 있다.
* SFT 
    * Supervised Fine-Tuning
    * 사람이 작성한 고품질 데이터로 모델을 fine-tuning 한다.
    * instruction-following 데이터셋을 사용한다. (질문-응답, 대화, 지시사항 등)
    * 특정 task에 대한 성능을 직접적으로 향상시킨다.
* RLHF
    * Reinforcement Learning from Human Feedback
    * 사람의 선호도를 반영한 피드백으로 모델을 최적화한다.
    * 모델을 보다 자연스럽고 일관성 있게 조정한다.
    * `보상 모델`을 먼저 학습하고 이를 사용하여 `정책 모델`을 학습한다.
    * `보상 모델`은 사전학습 모델의 아웃풋 레이어를 변경하여 점수 예측을 수행한다.
    * `정책 모델`은 사전학습 모델을 RLHF로 학습한 모델을 뜻하며, 응답을 생성한다.
    * 이 RLHF 학습 과정에서 `보상 모델`이 예측한 점수를 사용한다.
    * 실제 서비스에서는 `정책 모델`만 사용하여 응답을 생성한다.
    * `보상 모델`은 학습에서만 사용된다.
* 보통 학습 순서는 (사전학습 -> SFT -> RLHF) 순으로 진행된다.
<br><br>

### [LLM RLHF 워크플로우]
* Step 0: 준비
    * 사전학습과 SFT를 통해 초기 모델을 준비한다.
* Step 1: Human 데이터 수집
    * 초기 모델에 질문을 하여 2~5개의 응답을 생성한다.
    * 생성된 응답에 대해 사람이 선호도를 평가한다.
* Step 2: 보상 모델 학습
    * 사람의 피드백은 비용이 들기 때문에 계속 사용하기 힘들다.
    * 따라서 사람의 피드백을 예측하는 보상 모델을 학습한다.
    * 보상 모델은 질문과 응답 쌍을 입력으로 받아 선호도 점수를 예측한다.
* Step 3: 정책 모델 학습 (RL fine-tuning)
    * 새로운 질문에 대해 정책 모델이 응답을 생성한다.
    * 보상 모델이 이 응답에 대한 선호도 점수를 예측한다.
    * RL 알고리즘을 사용하여 높은 선호도의 응답을 생성하도록 학습한다.
<br><br>



## `[RLHF 알고리즘의 종류]`

### [PPO]
* Proximal Policy Optimization, 2017, OpenAI
* 핵심 아이디어: 강화학습을 통한 RLHF
* 인간 피드백으로 `보상 모델`을 학습한 뒤 `정책 모델`을 강화학습한다.
* 정책 업데이트 시 클리핑을 통해 안정성을 확보한다.
* SFT와 비교: 지도학습에서 강화학습으로 전환, 정책 업데이트 안정성 확보
* 문제점: 불안정한 학습, 높은 계산 비용, 복잡한 하이퍼파라미터 튜닝
* 수식 분석
    * 
* 데이터셋 예시
    * 
<br><br>

### [DPO]
* Direct Preference Optimization, 2023, 스탠포드
* 핵심 아이디어: 강화학습 없이 직접 선호도 최적화
* 보상 모델 없이 선호도 데이터를 직접 손실함수로 변환한다.
* 수학적으로 PPO와 동일하지만 훨씬 간단하게 구현할 수 있다.
* PPO와 비교: 강화학습에서 단순한 지도학습으로 전환, 안정적인 학습
* 문제점: 선호도 데이터의 양이 많아야 효과적
* 수식 분석
    * 
* 데이터셋 예시
    * 
<br><br>

### [IPO]
* Identity Preference Optimization, 2024, 구글
* 핵심 아이디어: DPO의 길이 편향 문제 해결
* DPO가 긴 응답을 선호하는 경향을 수정한다.
* 정규화 항 추가로 더 균형잡힌 최적화를 진행한다.
* DPO와 비교: 길이 편향 문제 해결, 더 안정적인 학습
* 문제점: 여전히 선호도 데이터의 양이 많아야 효과적
* 수식 분석
    * 
* 데이터셋 예시
    * 
<br><br>

### [KTO]
* Kahneman-Tversky Optimization, 2024, 스탠포드
* 핵심 아이디어: 쌍별 비교 대신 개별 피드백 활용
* "좋아요/싫어요"의 이진 피드백만으로 학습한다.
* 인간의 손실 회피 성향을 모델링한 것이다. (전망 이론 기반)
* IPO와 비교: 쌍별 선호 데이터 불필요, 데이터 효율성 향상
* 문제점: 이진 피드백의 낮은 정보량, 불안정성 등 (연구중)
* 수식 분석
    * 
* 데이터셋 예시
    * 
<br><br>

### [ORPO]
* Odds Ratio Preference Optimization, 2024, 카이스트
* 핵심 아이디어: SFT와 RLHF를 동시에 수행
* 별도의 SFT 단계 없이 한 번에 학습한다.
* 승산비(odds ratio)를 통한 선호도 모델링을 진행한다.
* PPO와 비교: 2단계 학습 -> 1단계 학습, 학습 효율성 향상
* 문제점: SFT 생략으로 초기 불안정, 복잡한 손실 함수 등 (연구중)
* 수식 분석
    * 
* 데이터셋 예시
    * 
<br><br>

### [GRPO]
* Group Relative Policy Optimization, 2024, 딥시크
* 핵심 아이디어: 그룹화된 상대적 순위 기반 최적화
* 여러 응답을 그룹으로 묶어 상대적 순위와 함께 학습한다.
* 리스트와이즈 학습 방식을 도입한다.
* IPO와 비교: 쌍별 데이터 -> 그룹별 데이터, 더 풍부한 선호 정보 활용
* 문제점: 그룹 데이터 수집 난이도, 계산 비용 등 (연구중)
* 수식 분석
    * 
* 데이터셋 예시
    * 
<br><br>