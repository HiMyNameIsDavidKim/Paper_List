# Attention Is All You Need
* VASWANI, Ashish, et al. Attention is all you need. Advances in neural information processing systems, 2017, 30.
* NIPS 2017
<br><br>

## [`논문 요약`]

### [저자의 의도]
* 길이가 긴 시퀸스는 기존 RNN에서 학습이 잘 안된다.
* 인코더와 디코더로 구성된 간단한 네트워크 아키텍쳐 제시. Transformer.
<br><br>

### [기존 문제점]
* 길이가 긴 시퀸스는 학습이 잘 안된다. (vanishing gradient)
* RNN 모델은 연속된 순서로 계산해야 한다.
* 병렬화 방해 발생
    * 순서대로 꺼내기 때문에 병렬 처리 안됨.
* 배치 문제 발생
    * 길이가 길면 메모리 문제가 있음.
    * 여러 개의 시퀸스를 처리할 수 없음.
* 결론적으로 느리다.
<br><br>

### [해결 아이디어]
* 긴 시퀸스를 처리하는데 어텐션 매커니즘이 아주 효율적.
* 어텐션 매커니즘을 기반으로 한 심플한 아키텍쳐의 모델 Transformer.
* scaled dot-product attention
    * self-attention의 한 종류.
    * 쿼리와 키의 내적을 키의 차원수 제곱근으로 스케일링.
    * (다른 위치 vs 자기 자신 위치) 간의 상호작용을 계산.
* multi-head attention
    * Q, K, V를 리니어 프로젝션 하여 공간 변환.
    * 병렬화 하여 scaled dot product 어텐션에 넣기.
    * 결과들을 concate로 합친 후 다시 프로젝션 하여 공간 복원.
    * 정보가 조각난 서브 공간 차원에서 representation을 병렬 계산할 수 있게 함.
    * 조각난 서브 공간 -> 다양한 측면에서 고려하므로 정확도 올라감.
    * 병렬 계산 -> 속도에서 효율적임.
    * 각 단어에 대한 정보를 추출하고, 단어간 간단한 관계 파악.
* position-wise feed-forward networks
    * 선형 변환 -> 렐루 -> 선형 변환 구조.
    * 선형변환을 Conv 1x1이라고 생각해도 무관하다.
    * 입력 시퀸스의 각 위치에서 연산을 수행해 위치마다 특성을 추출함.
    * 더 큰 차원인 히든레이어로 변환하여 연산하는 것.
    * 주변 단어와 조합해 단어간 복잡한 관계를 추가로 파악.
* 전체 구조에서 볼 때 어텐션의 3가지 역할.
    * 인코더의 입력값 내의 단어간 관계 파악.
    * 디코더의 출력값 내의 단어간 관계 파악.
    * 인코더-디코더의 입력값-출력값 단어간 관계 파악.
<br><br>

### [추가로 볼 레퍼런스]
* Layer Normalization
<br><br>

### [내 아이디어]
* 이미지 인식을 위해서는 2D 임베딩이 필요한게 아닌가.
    * 1D로 플래튼하면 상관 없다.
* 이미지 한개 이식 보다는 비디오에서 효과를 보이지 않을까.
    * 자율주행
* 어텐션 알고리즘이라는 알고리즘을 기반으로 한 딥러닝 아키텍쳐.
    * 그럼 전통적인 CV 알고리즘을 적용한 CNN 아키텍쳐를 만들면?
    * "전통적인 비전 기술이 적용된 딥러닝 아키텍쳐는 현재 존재하지 않습니다."
<br><br>



## [`메모`]

### [배경 지식]
* 인코더와 디코더는 기본적으로 어텐션 매커니즘에서 비롯된 것이다.
* ByteNet과 ConvS2S
    * 합성곱 레이어를 사용해 문제를 해결하려 했다.
    * 오히려 종속성 찾기 힘들었음.
* 셀프 어텐션
    * 한 시퀸스내에 representation 순서를 잘 계산한다.
    * 모든 단어를 임베딩해서 벡터로 변환.
    * 내적으로 문장 내의 각 단어 간 어텐션 스코어 계산.
* 임베딩 입력
    * 입력 문장 : ["The", "cat", "is", "cute"]
    * 임베딩 변환 : [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]
<br><br>
