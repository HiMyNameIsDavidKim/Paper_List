# Attention Is All You Need
* VASWANI, Ashish, et al. Attention is all you need. Advances in neural information processing systems, 2017, 30.
* NIPS 2017
* 책갈피 (3.4~)
<br><br>

## [`논문 요약`]

### [저자의 의도]
* RNN과 CNN에서 인코더와 디코더를 사용한다.
* 인코더와 디코더는 어텐션 매커니즘에서 비롯된 것이다.
* 어텐션 매커니즘으로만 구성된 간단한 네트워크 아키텍쳐 제시. Transformer.
<br><br>

### [기존 문제점]
* RNN 모델은 연속된 순서로 계산해야 한다.
* 병렬화 방해 발생
    * 순서대로 꺼내기 때문에 병렬 처리 안됨.
* 배치 문제 발생
    * 길이가 길면 메모리 문제가 있음.
    * 여러 개의 시퀸스를 처리할 수 없음.
* 결론적으로 느리다.
<br><br>

### [해결 아이디어]
* 긴 시퀸스를 처리하는데 어텐션 매커니즘이 아주 효율적.
* 어텐션 매커니즘을 기반으로 한 심플한 아키텍쳐의 모델 Transformer.
* multi-head self-attention
    * 역할은 각 단어에 대한 정보를 추출하고, 단어간 간단한 관계 파악.
    * Q, K, V를 리니어 프로젝션 하여 공간 변환.
    * 병렬화 하여 scaled dot product 어텐션에 넣기.
    * 결과들을 concate로 합친 후 다시 프로젝션 하여 공간 복원.
    * 정보가 조각난 서브 공간 차원에서 representation을 병렬 계산할 수 있게 함.
    * 조각난 서브 공간 -> 다양한 측면에서 고려하므로 정확도 올라감.
    * 병렬 계산 -> 속도에서 효율적임.
* position-wise feed-forward networks
    * 역할은 주변 단어와 조합해 단어간 복잡한 관계를 추가로 파악.
    * 선형 변환 -> 렐루 -> 선형 변환 구조.
    * 선형변환을 Conv 1x1이라고 생각해도 무관하다.
    * 입력 시퀸스의 각 위치에서 연산을 수행해 위치마다 특성을 추출함.
<br><br>

### [내가 사용할 부분]
* 
<br><br>

### [추가로 볼 레퍼런스]
* 
<br><br>



## [`메모`]

### [배경 지식]
* ByteNet과 ConvS2S
    * 합성곱 레이어를 사용해 문제를 해결하려 했다.
    * 오히려 종속성 찾기 힘들었음.
* 셀프 어텐션
    * 한 시퀸스내에 representation 순서를 잘 계산한다.
    * 모든 단어를 임베딩해서 벡터로 변환.
    * 내적으로 문장 내의 각 단어 간 어텐션 스코어 계산.
<br><br>
