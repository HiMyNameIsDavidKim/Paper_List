# ğŸ““Paper List
* Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation
* QIN, Yao, et al. Understanding and improving robustness of vision transformers through patch-based negative augmentation. Advances in Neural Information Processing Systems, 2022, 35: 16276-16289.
* NeurIPS 2022.
<br><br>

## [`ë…¼ë¬¸ ìš”ì•½ ì–‘ì‹`]

### [ì €ìì˜ ì˜ë„]
* ViTëŠ” ì—¬ì „íˆ non-robust í•˜ë‹¤.
* ViTì˜ robustnessì™€ í¼í¬ë¨¼ìŠ¤ë¥¼ í–¥ìƒ ì‹œí‚¬ ë°©ë²•ì€?
* patch-based negative augmentationì´ ViTì˜ robustnessì„ í–¥ìƒì‹œí‚¨ë‹¤.
<br><br>

### [ê¸°ì¡´ ë¬¸ì œì ]
* ViTëŠ” CNNë³´ë‹¤ëŠ” robustnessê°€ ì¢‹ì§€ë§Œ ì—¬ì „íˆ ì¢‹ì§€ ì•Šë‹¤.
<br><br>

### [í•´ê²° ì•„ì´ë””ì–´]
* ViTëŠ” ë†€ë¼ìš¸ ì •ë„ë¡œ patch-based transformationì— ë‘”ê°í•˜ë‹¤.
* ì´ ë‘”ê°ì„±ì´ non-robustë¥¼ ìœ ë°œí•˜ëŠ” ê²ƒì´ ì•„ë‹ê¹Œ í™•ì¸í•´ë³´ì.
* ì‚¬ëŒì´ ì¸ì§€í•  ìˆ˜ ì—†ëŠ” featureì— ëŒ€í•œ ViTì˜ ì˜ì¡´ì„±
    * ViTëŠ” patch-based transformationìœ¼ë¡œ semanticì´ íŒŒê´´ëœ ì´ë¯¸ì§€ë„ ë†’ì€ ì‹ ë¢°ë„ë¡œ ë§ì¶˜ë‹¤.
    * ì‚¬ëŒì€ ì „í˜€ ì•Œì•„ë³¼ ìˆ˜ ì—†ì„ ì •ë„ë¡œ íŒŒê´´í•´ë„ ë§ˆì°¬ê°€ì§€.
    * ViTê°€ ì–´ë–»ê²Œ ì´ë¯¸ì§€ë¥¼ ì¸ì§€í•˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ì—†ë‹¤.
* non-indicative featureì™€ robustnessì˜ ê´€ê³„
    * ì‹¤í—˜ì  ê²°ê³¼, patch-based transformationë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©í•´ì„œëŠ” ì•ˆëœë‹¤.
    * accuracyê°€ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ì…‹ë„ ë–¨ì–´ì§€ê³ , out-of-distribution ë°ì´í„°ì…‹ë„ ë–¨ì–´ì§.
* Negative augmentation
    * ViTê°€ non-indicative featureì— ì˜ì¡´í•˜ëŠ” ê²ƒì„ ê·œì œí•˜ê¸° ìœ„í•œ ë°©ë²•.
    * distribution labelsë¥¼ í†µí•´ í•™ìŠµë˜ëŠ” ê²ƒì„ ë°©í•´í•˜ë„ë¡ loss regularization.
    * ê¸°ì¡´ label ëŒ€ì‹  ë˜‘ê°™ì€ ê°’ì´ ë“¤ì–´ê°„ label ì‚¬ìš©.
    * (ex. [1, 0, 0, 0, 0] -> [0.2, 0.2, 0.2, 0.2, 0.2])
* ì‹¤í—˜ ê²°ê³¼
    * negative augmentationì„ í•´ë³´ë‹ˆ out-of distribution ë°ì´í„°ì…‹ì—ì„œ ì •í™•ë„ê°€ ì˜¬ë¼ê°.
    * positive augmentationê³¼ ê°™ì´ ì§„í–‰í•´ë„ ì˜¬ë¼ê°. ìƒí˜¸ë³´ì™„ì ì„.
<br><br>

### [ì¶”ê°€ë¡œ ë³¼ ë ˆí¼ëŸ°ìŠ¤]
* 
<br><br>

### [ë‚´ ì•„ì´ë””ì–´]
* 
<br><br>



## [`ë©”ëª¨`]

### [ë°°ê²½ ì§€ì‹]
* robustness
    * ëª¨ë¸ì´ ì…ë ¥ì˜ ë‹¤ì–‘í•œ ë³€í™”ì™€ ë…¸ì´ì¦ˆì— ì–¼ë§ˆë‚˜ ê²¬ê³ í•œì§€ì— ëŒ€í•œ ì§€í‘œ.
    * NLPì—ì„œ ë„ì–´ì“°ê¸°, ì˜¤íƒ€ ë“±ì´ ìˆì–´ë„ ì •í™•í•˜ê²Œ ë²ˆì—­ë˜ëŠ” ê²ƒ.
    * ViTëŠ” pre-train datasetì´ í´ìˆ˜ë¡ robustnessí•¨.
    * ViTëŠ” CNNê³„ì—´ê³¼ ë¹„êµí•˜ì—¬ robustnessê°€ ì¢‹ì€ ì¢…ë¥˜ê°€ ë‹¤ë¦„.
* ê³ ë„ë¡œ ì‘ì€ íŒ¨ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ViTëŠ” í”½ì…€ ì£¼ë³€ì„ íƒìƒ‰í•˜ëŠ” CNNê³¼ ìœ ì‚¬í•œ íŠ¹ì§•ì´ ìˆë‹¤.
* semantic-preserving augmentation
    * ì¦ê°•ì„ ìœ„í•´ ë³€í˜•ëœ ë°ì´í„°ë„ ì›ë˜ ë°ì´í„°ì™€ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°€ì ¸ì•¼ë§Œ í•œë‹¤.
* ImageNet out-of-distribution dataset
    * ImageNet-A
        * real-world ì´ë¯¸ì§€. ResNetì´ êµ¬ë³„í•˜ì§€ ëª»í•œ ì´ë¯¸ì§€ë“¤ë§Œ ëª¨ì•„ë†“ì€ ê²ƒ.
        * ë„¤íŠ¸ì›Œí¬ì˜ ì·¨ì•½ì„±ì„ ì§‘ì¤‘ì ìœ¼ë¡œ í™•ì¸.
    * ImageNet-C
        * íšŒì „, ë¸”ëŸ¬, ë…¸ì´ì¦ˆ, í•´ìƒë„ ë“œë, ê·¸ë¦¼ì ë“±.
        * ì˜ ì•Œë ¤ì§„ robustness í‰ê°€.
    * ImageNet-R
        * ë§Œí™”, ê·¸ë¦¼, ì˜ˆìˆ , íƒ€íˆ¬, ì• ë‹ˆë©”ì´ì…˜ ë“±.
        * ë‹¤ë¥¸ ë¶„í¬ì˜ ë°ì´í„°ì—ì„œë„ ì˜ ë™ì‘í•˜ëŠ”ì§€ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í‰ê°€.
<br><br>


